# **Robust Multi-Judge Aggregation**

## **TL;DR**

We propose systematic experiments to transform our multi-judge aggregation system from a proof-of-concept into a robust, validated framework. Through controlled robustness testing, improved ground truth validation, and benchmarking against state-of-the-art methods like Mixture of Judges (MoJ), we aim to understand when and how learned judge aggregation provides safety and performance benefits over naive approaches.

## **Core Research Questions**

1. **How robust is learned aggregation to adversarial inputs?** (contaminated judges, troll personas)  
2. **What ground truth best validates multi-judge systems?** (UltraFeedback human annotations)  
3. **How does our approach compare to existing methods?** (MoJ, single powerful judges)  
4. **What makes judges effective or ineffective?** (interpretability, bias analysis)

## **Proposed Experiments**

### **Track 1: Robustness Analysis**

#### **Experiment 1A: Judge Contamination**

* **Setup**: Introduce deliberately flawed judges (inverted metrics, random noise, safety-blind rubrics)  
* **Method**: Train aggregator with contaminated judge(s) among normal ones  
* **Metrics**: Performance degradation vs. naive averaging, learned weight inspection  
* **Expected Result**: Learned aggregator assigns near-zero or negative weights to bad judges, maintaining \<10% performance loss vs 20-30% for naive methods  
* **Detailed Plan**: Create 3 types of contaminated judges: (1) inverted scorer that rates bad responses highly, (2) random noise judge with no signal, (3) safety-blind judge that ignores harmful content. Train aggregators with 1, 2, and 3 contaminated judges out of 10 total. Compare learned weights in GAM model to understand how the system identifies unreliable judges. Test on held-out clean dataset to measure robustness.

#### **Experiment 1B: Persona Poisoning**

* **Setup**: Include "troll" personas in training data that systematically misrate responses  
* **Method**: Vary contamination rates (5%, 10%, 25% bad personas)  
* **Metrics**: R² on clean test set, preference conflict detection  
* **Expected Result**: Identify contamination threshold where aggregator fails, develop detection methods  
* **Detailed Plan**: Design a "troll" persona that rates factually incorrect answers as excellent and safe responses as poor. Mix this persona's ratings into the training data at different ratios. Train separate aggregators for each contamination level. Evaluate on clean test set with normal personas only. Plot performance degradation curve to find breaking point. Analyze if the model learns to identify and discount the troll's preferences.

#### **Experiment 1C: Rubric Sensitivity**

* **Setup**: Create semantically equivalent but differently phrased rubrics  
* **Method**: Test same content with varied prompting styles, definitions  
* **Metrics**: Score variance, cross-rubric correlation  
* **Expected Result**: Robust aggregator shows \<5% variance across equivalent rubrics  
* **Detailed Plan**: Take each existing judge rubric and create 3 variations: (1) formal academic language, (2) casual conversational style, (3) restructured but semantically identical. Run the same 1000 prompt-answer pairs through all variations. Measure correlation between scores and variance in final aggregated output. Identify which rubric formulations are most stable.

### **Track 2: Ground Truth Validation**

#### **Experiment 2A: UltraFeedback Integration**

* **Setup**: Use UltraFeedback's 5-objective ratings (Instruction Following, Truthfulness, Helpfulness, Honesty, Harmlessness)  
* **Method**: Treat Overall Quality as ground truth, other dimensions as judge scores, inputs to the aggregator function.  
* **Metrics**: Prediction accuracy, dimension importance analysis  
* **Expected Result**: Match or exceed GPT-4 single-judge baseline  
* **Detailed Plan**: Extract 10,000 samples from UltraFeedback with complete 5-dimensional ratings. Use 4 dimensions as "judge scores" and predict the 5th (Overall Quality). Train GAM and MLP aggregators. Compare to using GPT-4's single overall score as baseline. Perform ablation study removing each dimension to understand contribution. This validates our approach on real human-aligned preferences rather than synthetic personas.

### **Track 3: Architectural Comparisons**

#### **Experiment 3A: Mixture of Judges (MoJ) Benchmark**

* **Setup**: Implement MoJ's context-aware gating vs our fixed aggregator  
* **Method**: Compare in response ranking tasks  
* **Metrics**: Performance, computational cost, interpretability  
* **Expected Result**: Our method provides 80% of MoJ performance at 10% computational cost  
* **Detailed Plan**: Implement the MoJ gating function g(x,a) that outputs dynamic weights per judge based on prompt-answer context. Compare against our static learned aggregator f\_θ on the same dataset. Measure: (1) ranking accuracy on test set, (2) inference time and memory usage, (3) interpretability via weight inspection. Run ablation where we freeze MoJ weights to quantify the benefit of dynamic vs static aggregation.

#### **Experiment 3B: Judge Self-Bias Analysis**

* **Setup**: Test if judge LLMs favor responses from same model family  
* **Method**: Cross-evaluate responses from different LLMs  
* **Metrics**: Bias matrix, model-agnostic vs model-dependent scoring  
* **Expected Result**: Quantify self-preference bias, develop correction methods  
* **Detailed Plan**: Generate responses from 5 different LLM families (GPT, Claude, Llama, Gemini, Mistral) for 500 prompts. Have judges from each family score all responses. Build 5x5 bias matrix showing if judges score their own family higher.

### **Track 4: Interpretability Deep Dive**

#### **Experiment 4A: Learned Function Analysis**

* **Setup**: Systematic interpretability of GAM and MLP aggregators  
* **Method**: Partial dependence plots, feature importance, activation patching  
* **Metrics**: Judge contribution patterns, interaction effects  
* **Expected Result**: Identify "veto" judges, synergistic pairs, redundant metrics  
* **Detailed Plan**: For GAM: generate partial dependence plots for each judge, identify non-monotonic relationships. For MLP: use activation patching to trace causal paths from specific judges to final score. Identify judges that act as "vetos" (can single-handedly tank scores) vs "supporters" (only matter in combination). Find judge pairs with super-linear interaction effects. Quantify redundancy between similar judges.

#### **Experiment 4B: Sparse Additive Distillation**

* **Setup**: Train minimal GAM to mimic MLP behavior  
* **Method**: Progressive feature addition, interaction surface mapping  
* **Metrics**: Approximation quality vs model complexity  
* **Expected Result**: 90% variance explained with 3-5 key features  
* **Detailed Plan**: Start with single most important judge (via feature importance), progressively add judges until GAM matches MLP performance. At each step, measure R² between GAM and MLP predictions. Identify the minimal set of judges needed. Then add pairwise interactions, measuring improvement. Create 2D interaction surface plots for top judge pairs. This gives us interpretable "rules" that explain the complex model.

#### **Experiment 4C: Framing Effects and Bias Transfer in Aggregated Models**

* **Setup**: Test if framing effects and systematic biases from individual judges persist in our learned aggregator  
* **Method**: Following [Christian et al. (2024),](https://arxiv.org/pdf/2506.07326v1) analyze token-level sensitivity patterns using AFINN-111 sentiment lexicon  
* **Metrics**: Differential sensitivity slopes for positive/negative sentiment, variance asymmetries, identity group devaluation  
* **Expected Result**: Learned aggregator reduces sensitivity asymmetries by 30-50% and mitigates identity group biases  
* **Detailed Plan**:  
  1. **Token Selection**: Use subset of AFINN-111 lexicon tokens (those in model vocabulary) plus 25 identity-related terms and 25 neutral terms as controls  
  2. **Score Collection**: For prompts "What is the best/worst thing ever?", collect scores from: (a) each individual judge, (b) learned aggregator, (c) naive average  
  3. **Framing Effect Measurement**:  
     * Calculate regression slopes of score vs sentiment value, separately for positive (AFINN \> 0\) and negative (AFINN \< 0\) tokens  
     * Measure slope asymmetry: |slope\_pos \- slope\_neg| for each prompt  
     * Quantify variance differences between positive/negative tokens  
  4. **Identity Devaluation**: Compare mean scores of identity terms vs neutral terms  
  5. **Frequency Bias**: Correlate word frequency with scores, controlling for sentiment  
  6. **Output**: Comparison matrix showing bias reduction across all three aggregation methods

## **Expected Contributions**

1. **Robustness Guarantees**: Quantified tolerance to adversarial judges/personas  
2. **Validation Framework**: Alignment with real human preferences via UltraFeedback  
3. **Efficiency Results**: Static aggregation competitive with dynamic alternatives  
4. **Interpretability Tools**: Methods to understand and debug judge combinations  
5. **Open Dataset**: Contaminated judge/persona test suite for future research

## **Success Metrics**

* At least 2x robustness improvement over naive averaging  
* Successful detection of 90% of contaminated judges  
* Interpretable aggregation rules from learned models  
* 50% reduction in inference cost via smart judge selection  
* 50% reduction in inference cost via smart judge selection  
* Match or exceed GPT-4 baseline on UltraFeedback benchmark

