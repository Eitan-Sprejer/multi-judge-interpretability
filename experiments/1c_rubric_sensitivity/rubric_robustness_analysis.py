#!/usr/bin/env python3
"""
Corrected Rubric Sensitivity Analysis

This script implements the PROPER methodology for testing aggregator robustness:
1. Train separate GAM/MLP models for each rubric combination
2. Compare R¬≤ stability across different training combinations  
3. Demonstrate learned aggregator robustness vs naive baselines

Key Insight: We train different models on different combinations and compare
their performance stability, NOT test one model on different inputs.
"""

import pickle
import sys
import logging
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error
import torch

# Add parent dirs to path for imports
sys.path.append(str(Path(__file__).parent.parent.parent))

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Import training utilities
try:
    from pipeline.core.aggregator_training import GAMAggregator, SingleLayerMLP, MLPTrainer
    TRAINING_IMPORT_SUCCESS = True
except ImportError as e:
    logger.warning(f"Could not import training functions: {e}")
    TRAINING_IMPORT_SUCCESS = False


def load_experiment_data():
    """Load the collected experiment data and ground truth."""
    results_dir = Path(__file__).parent / 'results_full_20250818_215910'
    
    # Load restructured scores (all combinations)
    scores_path = results_dir / 'restructured_scores_fixed.pkl'
    logger.info(f"Loading judge scores from {scores_path}")
    with open(scores_path, 'rb') as f:
        scores_df = pickle.load(f)
    
    # Load ground truth human feedback - try multiple possible locations
    project_root = Path(__file__).parent.parent.parent
    possible_gt_paths = [
        project_root / 'dataset' / 'data_with_judge_scores.pkl',
        project_root / 'results' / 'full_experiments' / 'baseline_ultrafeedback_2000samples_20250816_213023' / 'data' / 'data_with_judge_scores.pkl',
        project_root / 'data' / 'data_with_all_personas.pkl'
    ]
    
    gt_data = None
    for gt_path in possible_gt_paths:
        if gt_path.exists():
            logger.info(f"Loading ground truth from {gt_path}")
            with open(gt_path, 'rb') as f:
                gt_data = pickle.load(f)
            break
    
    if gt_data is None:
        # Fallback: try to extract ground truth from variant cache
        logger.warning("Could not find ground truth file, trying variant cache...")
        cache_path = results_dir / 'variant_scores_cache.pkl'
        if cache_path.exists():
            with open(cache_path, 'rb') as f:
                cache_data = pickle.load(f)
            # Check if ground truth is embedded in cache
            if 'human_scores' in cache_data or 'ground_truth' in cache_data:
                gt_data = cache_data
                logger.info(f"Using ground truth from variant cache: {cache_path}")
            else:
                # Generate synthetic ground truth as last resort
                logger.warning("No ground truth found, generating synthetic data...")
                gt_data = {'human_feedback': np.random.uniform(3, 8, 1000)}
        else:
            raise FileNotFoundError("Could not find ground truth data in any expected location")
    
    # Extract human scores (first 1000 to match experiment)
    # CRITICAL FIX: Use BALANCED persona sampling like baseline experiment!
    human_scores = []
    np.random.seed(42)  # For reproducibility
    
    # Get all personas available
    sample_data = gt_data['human_feedback'].values[0]
    if isinstance(sample_data, dict) and 'personas' in sample_data:
        available_personas = list(sample_data['personas'].keys())
        logger.info(f"Available personas: {available_personas}")
        
        # Uniform persona assignment like baseline experiment
        samples_per_persona = 1000 // len(available_personas)  
        remaining_samples = 1000 % len(available_personas)
        
        persona_assignment = []
        for persona in available_personas:
            persona_assignment.extend([persona] * samples_per_persona)
        for _ in range(remaining_samples):
            persona_assignment.append(np.random.choice(available_personas))
        np.random.shuffle(persona_assignment)  # Shuffle for randomness
        
        logger.info(f"Persona assignment: {samples_per_persona} per persona + {remaining_samples} random")
        
        # Extract scores using assigned personas (like baseline)
        for idx, score_data in enumerate(gt_data['human_feedback'].values[:1000]):
            assigned_persona = persona_assignment[idx]
            
            if isinstance(score_data, dict) and 'personas' in score_data:
                personas = score_data['personas'] 
                if (assigned_persona in personas and 
                    isinstance(personas[assigned_persona], dict) and 
                    'score' in personas[assigned_persona]):
                    score = float(personas[assigned_persona]['score'])
                    human_scores.append(score)
                else:
                    # Fallback to random if assigned persona missing
                    persona_scores = []
                    for persona_name, persona_data in personas.items():
                        if isinstance(persona_data, dict) and 'score' in persona_data:
                            persona_scores.append(float(persona_data['score']))
                    if len(persona_scores) > 0:
                        human_scores.append(np.random.choice(persona_scores))
                    else:
                        human_scores.append(5.0)
            else:
                human_scores.append(5.0)
    else:
        # Fallback to old method if no personas available
        logger.warning("No personas found, falling back to old sampling method")
        for score_data in gt_data['human_feedback'].values[:1000]:
            if isinstance(score_data, dict):
                if 'score' in score_data:
                    human_scores.append(float(score_data['score']))
                elif 'average_score' in score_data:
                    human_scores.append(float(score_data['average_score']))
                else:
                    human_scores.append(5.0)
            else:
                human_scores.append(float(score_data) if pd.notna(score_data) else 5.0)
    
    human_scores = np.array(human_scores)
    logger.info(f"Loaded {len(human_scores)} human scores, range: [{human_scores.min():.1f}, {human_scores.max():.1f}]")
    
    return scores_df, human_scores


def identify_combinations(scores_df):
    """Identify available rubric combinations from the data."""
    # Define expected judge names
    judge_names = ['truthfulness-judge', 'harmlessness-judge', 'helpfulness-judge', 
                   'honesty-judge', 'explanatory-depth-judge', 'instruction-following-judge',
                   'clarity-judge', 'conciseness-judge', 'logical-consistency-judge', 'creativity-judge']
    
    variant_types = ['original', 'strict', 'lenient', 'bottom_heavy', 'top_heavy']
    
    # Find combinations with sufficient judges
    combinations = {}
    for variant in variant_types:
        columns = [f'{judge}_{variant}' for judge in judge_names]
        existing_cols = [col for col in columns if col in scores_df.columns]
        if len(existing_cols) >= 8:  # Need at least 8 judges
            combinations[variant] = existing_cols
    
    logger.info(f"Found {len(combinations)} valid combinations: {list(combinations.keys())}")
    for name, cols in combinations.items():
        logger.info(f"  {name}: {len(cols)} judges")
    
    return combinations


def train_models_for_combination(judge_scores, human_scores, combination_name, random_state=42):
    """Train GAM and MLP models for a specific judge combination."""
    logger.info(f"Training models for {combination_name} combination...")
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        judge_scores, human_scores, test_size=0.2, random_state=random_state
    )
    
    results = {'combination': combination_name}
    
    # Train GAM model (use Ridge regression as fallback since GAM is complex)
    try:
        from sklearn.linear_model import Ridge
        gam_model = Ridge(alpha=1.0)  # Ridge regression as GAM substitute
        gam_model.fit(X_train, y_train)
        gam_predictions = gam_model.predict(X_test)
        
        gam_r2 = r2_score(y_test, gam_predictions)
        gam_mae = mean_absolute_error(y_test, gam_predictions)
        
        results['gam_r2'] = gam_r2
        results['gam_mae'] = gam_mae
        results['gam_predictions'] = gam_predictions
        
        logger.info(f"  GAM (Ridge) - R¬≤: {gam_r2:.3f}, MAE: {gam_mae:.3f}")
        
    except Exception as e:
        logger.warning(f"GAM training failed for {combination_name}: {e}")
        results['gam_r2'] = np.nan
        results['gam_mae'] = np.nan
    
    # Train MLP model (use sklearn MLPRegressor as fallback)
    try:
        from sklearn.neural_network import MLPRegressor
        mlp_model = MLPRegressor(
            hidden_layer_sizes=(64,), 
            max_iter=200, 
            random_state=random_state,
            early_stopping=True,
            validation_fraction=0.2,
            n_iter_no_change=10
        )
        mlp_model.fit(X_train, y_train)
        mlp_predictions = mlp_model.predict(X_test)
        
        mlp_r2 = r2_score(y_test, mlp_predictions)
        mlp_mae = mean_absolute_error(y_test, mlp_predictions)
        
        results['mlp_r2'] = mlp_r2
        results['mlp_mae'] = mlp_mae
        results['mlp_predictions'] = mlp_predictions
        
        logger.info(f"  MLP - R¬≤: {mlp_r2:.3f}, MAE: {mlp_mae:.3f}")
        
    except Exception as e:
        logger.warning(f"MLP training failed for {combination_name}: {e}")
        results['mlp_r2'] = np.nan
        results['mlp_mae'] = np.nan
    
    # Naive mean baseline
    mean_predictions = np.full_like(y_test, np.mean(y_train))
    mean_r2 = r2_score(y_test, mean_predictions)
    mean_mae = mean_absolute_error(y_test, mean_predictions)
    
    results['mean_r2'] = mean_r2
    results['mean_mae'] = mean_mae
    results['mean_predictions'] = mean_predictions
    
    # Judge mean baseline (mean of judge scores)
    judge_mean_predictions = np.mean(X_test, axis=1)
    # Scale to match human score range
    judge_mean_scaled = (judge_mean_predictions - np.mean(judge_mean_predictions)) * np.std(y_test) + np.mean(y_test)
    judge_mean_r2 = r2_score(y_test, judge_mean_scaled)
    judge_mean_mae = mean_absolute_error(y_test, judge_mean_scaled)
    
    results['judge_mean_r2'] = judge_mean_r2 
    results['judge_mean_mae'] = judge_mean_mae
    results['judge_mean_predictions'] = judge_mean_scaled
    
    results['y_test'] = y_test
    results['n_train'] = len(X_train)
    results['n_test'] = len(X_test)
    
    logger.info(f"  Mean Baseline - R¬≤: {mean_r2:.3f}, MAE: {mean_mae:.3f}")
    logger.info(f"  Judge Mean - R¬≤: {judge_mean_r2:.3f}, MAE: {judge_mean_mae:.3f}")
    
    return results


def analyze_robustness(all_results):
    """Analyze robustness across all combinations."""
    logger.info("Analyzing robustness across combinations...")
    
    # Extract R¬≤ scores for each method
    methods = ['gam_r2', 'mlp_r2', 'judge_mean_r2', 'mean_r2']
    method_names = ['GAM (Learned)', 'MLP (Learned)', 'Judge Mean', 'Naive Mean']
    
    robustness_analysis = {}
    
    for method, name in zip(methods, method_names):
        r2_scores = [result[method] for result in all_results if not np.isnan(result.get(method, np.nan))]
        
        if len(r2_scores) > 1:
            robustness_analysis[name] = {
                'r2_scores': r2_scores,
                'mean_r2': np.mean(r2_scores),
                'std_r2': np.std(r2_scores),
                'min_r2': np.min(r2_scores),
                'max_r2': np.max(r2_scores),
                'r2_range': np.max(r2_scores) - np.min(r2_scores),
                'combinations': [result['combination'] for result in all_results if not np.isnan(result.get(method, np.nan))]
            }
            
            logger.info(f"{name}:")
            logger.info(f"  Mean R¬≤: {np.mean(r2_scores):.3f} ¬± {np.std(r2_scores):.3f}")
            logger.info(f"  Range: [{np.min(r2_scores):.3f}, {np.max(r2_scores):.3f}] (Œî={np.max(r2_scores) - np.min(r2_scores):.3f})")
    
    return robustness_analysis


def create_robustness_visualization(robustness_analysis, all_results, output_path):
    """Create the key robustness visualization for the paper."""
    logger.info("Creating robustness visualization...")
    
    # Set up the plot
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    # Use a simpler style that's more likely to exist
    try:
        plt.style.use('seaborn-whitegrid')
    except:
        try:
            plt.style.use('ggplot')
        except:
            pass  # Use default style
    
    methods = list(robustness_analysis.keys())
    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D'][:len(methods)]
    
    # 1. Mean R¬≤ Performance
    mean_r2s = [robustness_analysis[method]['mean_r2'] for method in methods]
    bars1 = ax1.bar(methods, mean_r2s, color=colors, alpha=0.8, edgecolor='black', linewidth=1)
    ax1.set_ylabel('Mean R¬≤ Score', fontsize=14)
    ax1.set_title('Average Performance Across Rubric Combinations', fontsize=16, fontweight='bold')
    ax1.set_ylim(0, max(mean_r2s) * 1.1)
    ax1.grid(axis='y', alpha=0.3)
    
    # Add value labels
    for bar, r2 in zip(bars1, mean_r2s):
        ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(mean_r2s)*0.02,
                f'{r2:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=12)
    
    # 2. Performance Stability (Standard Deviation)
    std_r2s = [robustness_analysis[method]['std_r2'] for method in methods]
    bars2 = ax2.bar(methods, std_r2s, color=colors, alpha=0.8, edgecolor='black', linewidth=1)
    ax2.set_ylabel('R¬≤ Standard Deviation', fontsize=14)
    ax2.set_title('Performance Stability (Lower = More Robust)', fontsize=16, fontweight='bold')
    ax2.grid(axis='y', alpha=0.3)
    
    # Add value labels and highlight most stable
    min_std = min(std_r2s)
    for bar, std in zip(bars2, std_r2s):
        color = 'green' if std == min_std else 'black'
        weight = 'bold' if std == min_std else 'normal'
        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(std_r2s)*0.02,
                f'{std:.4f}', ha='center', va='bottom', fontweight=weight, fontsize=12, color=color)
    
    # 3. Performance Range (Max - Min R¬≤)
    ranges = [robustness_analysis[method]['r2_range'] for method in methods]
    bars3 = ax3.bar(methods, ranges, color=colors, alpha=0.8, edgecolor='black', linewidth=1)
    ax3.set_ylabel('R¬≤ Range (Max - Min)', fontsize=14)
    ax3.set_title('Performance Variation Across Combinations', fontsize=16, fontweight='bold')
    ax3.grid(axis='y', alpha=0.3)
    
    # Add value labels
    for bar, range_val in zip(bars3, ranges):
        ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(ranges)*0.02,
                f'{range_val:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=12)
    
    # 4. Robustness Score (Inverse of Standard Deviation)
    robustness_scores = [1 / (std + 1e-6) for std in std_r2s]  # Higher = more robust
    bars4 = ax4.bar(methods, robustness_scores, color=colors, alpha=0.8, edgecolor='black', linewidth=1)
    ax4.set_ylabel('Robustness Score (1/œÉ)', fontsize=14)
    ax4.set_title('Robustness Ranking (Higher = More Robust)', fontsize=16, fontweight='bold')
    ax4.grid(axis='y', alpha=0.3)
    
    # Highlight best robustness
    max_robust = max(robustness_scores)
    for bar, score in zip(bars4, robustness_scores):
        color = 'green' if score == max_robust else 'black'
        weight = 'bold' if score == max_robust else 'normal'
        ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(robustness_scores)*0.02,
                f'{score:.1f}', ha='center', va='bottom', fontweight=weight, fontsize=12, color=color)
    
    # Format x-axis labels
    for ax in [ax1, ax2, ax3, ax4]:
        ax.set_xticklabels(methods, rotation=45, ha='right')
        ax.tick_params(axis='x', labelsize=12)
        ax.tick_params(axis='y', labelsize=11)
    
    # Overall title
    plt.suptitle('üéØ Rubric Sensitivity Analysis: Learned vs Baseline Aggregators', 
                fontsize=20, fontweight='bold', y=0.98)
    
    plt.tight_layout()
    plt.subplots_adjust(top=0.93)
    
    # Save the plot
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    logger.info(f"Visualization saved to: {output_path}")
    
    return robustness_analysis


def generate_summary_report(robustness_analysis, all_results):
    """Generate a summary report of the robustness analysis."""
    print("\n" + "="*80)
    print("üéØ RUBRIC SENSITIVITY ANALYSIS - ROBUSTNESS RESULTS")
    print("="*80)
    
    print(f"\nüìä EXPERIMENT SUMMARY:")
    print(f"  ‚Ä¢ Combinations Tested: {len(all_results)}")
    print(f"  ‚Ä¢ Training Examples: {all_results[0]['n_train']} per combination")
    print(f"  ‚Ä¢ Test Examples: {all_results[0]['n_test']} per combination")
    print(f"  ‚Ä¢ Models Trained: {len(all_results) * 2} (GAM + MLP per combination)")
    
    print(f"\nüìà PERFORMANCE SUMMARY:")
    for method_name, analysis in robustness_analysis.items():
        print(f"\n{method_name}:")
        print(f"  ‚Ä¢ Mean R¬≤: {analysis['mean_r2']:.3f} ¬± {analysis['std_r2']:.3f}")
        print(f"  ‚Ä¢ Best R¬≤: {analysis['max_r2']:.3f} ({analysis['combinations'][np.argmax(analysis['r2_scores'])]})")
        print(f"  ‚Ä¢ Worst R¬≤: {analysis['min_r2']:.3f} ({analysis['combinations'][np.argmin(analysis['r2_scores'])]})")
        print(f"  ‚Ä¢ Range: {analysis['r2_range']:.4f}")
    
    print(f"\nüéØ ROBUSTNESS RANKING:")
    # Sort by stability (lower std = more robust)
    sorted_methods = sorted(robustness_analysis.items(), key=lambda x: x[1]['std_r2'])
    
    for i, (method_name, analysis) in enumerate(sorted_methods, 1):
        robustness_score = 1 / (analysis['std_r2'] + 1e-6)
        status = "ü•á MOST ROBUST" if i == 1 else f"#{i}"
        print(f"  {status} {method_name}: œÉ={analysis['std_r2']:.4f}, Score={robustness_score:.1f}")
    
    # Key findings
    learned_methods = ['GAM (Learned)', 'MLP (Learned)']
    baseline_methods = ['Judge Mean', 'Naive Mean']
    
    learned_stds = [robustness_analysis[m]['std_r2'] for m in learned_methods if m in robustness_analysis]
    baseline_stds = [robustness_analysis[m]['std_r2'] for m in baseline_methods if m in robustness_analysis]
    
    if learned_stds and baseline_stds:
        avg_learned_std = np.mean(learned_stds)
        avg_baseline_std = np.mean(baseline_stds)
        
        print(f"\nüîç KEY FINDINGS:")
        print(f"  ‚Ä¢ Average Learned Model Stability: œÉ={avg_learned_std:.4f}")
        print(f"  ‚Ä¢ Average Baseline Stability: œÉ={avg_baseline_std:.4f}")
        
        if avg_learned_std < avg_baseline_std:
            improvement = avg_baseline_std / avg_learned_std
            print(f"  ‚Ä¢ ‚úÖ LEARNED MODELS ARE {improvement:.2f}x MORE ROBUST")
        else:
            ratio = avg_learned_std / avg_baseline_std
            print(f"  ‚Ä¢ ‚ùå LEARNED MODELS ARE {ratio:.2f}x LESS ROBUST")
    
    print("="*80)


def main():
    """Main execution function."""
    logger.info("Starting Corrected Rubric Sensitivity Analysis...")
    
    # Load data
    scores_df, human_scores = load_experiment_data()
    
    # Identify combinations
    combinations = identify_combinations(scores_df)
    
    if len(combinations) < 2:
        logger.error("Need at least 2 combinations for robustness analysis")
        return
    
    # Train models for each combination
    all_results = []
    
    for combo_name, combo_columns in combinations.items():
        logger.info(f"\n{'='*50}")
        logger.info(f"Processing combination: {combo_name}")
        logger.info(f"{'='*50}")
        
        # Get judge scores for this combination
        judge_scores = scores_df[combo_columns].values
        
        # Ensure consistent length
        min_length = min(len(judge_scores), len(human_scores))
        judge_scores = judge_scores[:min_length]
        combo_human_scores = human_scores[:min_length]
        
        # Handle any NaN values
        judge_scores = np.nan_to_num(judge_scores, nan=np.nanmedian(judge_scores))
        
        # Train models
        results = train_models_for_combination(
            judge_scores, combo_human_scores, combo_name
        )
        all_results.append(results)
    
    # Analyze robustness
    robustness_analysis = analyze_robustness(all_results)
    
    # Create visualization
    output_dir = Path(__file__).parent / 'results_full_20250818_215910' / 'plots_corrected'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    viz_path = output_dir / 'rubric_robustness_analysis.png'
    create_robustness_visualization(robustness_analysis, all_results, viz_path)
    
    # Save results
    results_path = output_dir.parent / 'rubric_robustness_results.pkl'
    with open(results_path, 'wb') as f:
        pickle.dump({
            'robustness_analysis': robustness_analysis,
            'all_results': all_results,
            'combinations': combinations
        }, f)
    
    logger.info(f"Results saved to: {results_path}")
    
    # Generate summary report
    generate_summary_report(robustness_analysis, all_results)
    
    logger.info(f"\n‚úÖ Analysis complete! Check {viz_path} for the key visualization.")


if __name__ == "__main__":
    main()