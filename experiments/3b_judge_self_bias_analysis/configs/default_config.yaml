# Default configuration for Experiment 3b: Multi-LLM Judge Creation

# LLM Provider Configuration
llm_providers:
  openai:
    name: "OpenAI"
    model: "gpt-4o-mini"
    description: "GPT-4o-mini based judge"
  anthropic:
    name: "Anthropic"
    model: "claude-3-5-sonnet"
    description: "Claude-3.5-sonnet based judge"
  google:
    name: "Google"
    model: "gemini-1.5-flash"
    description: "Gemini-1.5-flash based judge"
  together:
    name: "Together"
    model: "llama-3.1-70b"
    description: "Llama-3.1-70B based judge"
  meta:
    name: "Meta"
    model: "llama-3.1-8b"
    description: "Llama-3.1-8B based judge"

# Judge Configuration
judge_config:
  min_score: 0.0
  max_score: 4.0
  scoring_precision: 1

# Rubric Types (from pipeline/utils/judge_rubrics.py)
rubric_types:
  - harmlessness-judge
  - privacy-judge
  - factual-accuracy-judge
  - prompt-faithfulness-relevance-judge
  - calibration-uncertainty-judge
  - bias-fairness-judge
  - reasoning-consistency-judge
  - discourse-coherence-judge
  - conciseness-redundancy-judge
  - style-formatting-judge

# Expected Output
expected_output:
  total_judges: 50
  judge_naming: "{provider}-{rubric}"

# Logging Configuration
logging:
  level: "INFO"
  file: "judge_creation.log"
  format: "%(asctime)s - %(levelname)s - %(message)s"

# API Configuration
api:
  base_url: "https://withmartian.com/api"
  timeout: 30
  max_retries: 3
  retry_delay: 1.0

# Output Configuration
output:
  save_judge_list: true
  judge_list_file: "created_judges.json"
  summary_report: true
  report_file: "judge_creation_summary.md"
